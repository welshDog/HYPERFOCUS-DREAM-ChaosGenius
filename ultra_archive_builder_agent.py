#!/usr/bin/env python3
"""
ğŸ’¾ğŸ§¬ ULTRA ARCHIVE BUILDER AGENT ğŸ§¬ğŸ’¾
ğŸŒŒ Master Core System for Intelligent Archiving & Compression ğŸŒŒ
ğŸ‘‘ By Command of Chief Lyndz - LEGENDARY FILE MASTERY! ğŸ‘‘

ğŸ”¥ FEATURES:
   â€¢ Auto-shrink directories by rules
   â€¢ Tagging system (#hyperdoc, #fast, #encrypt)
   â€¢ Metadata storage (size, SHA256, timestamps)
   â€¢ Version control with replay/restore
   â€¢ Compression presets (Fast, Legendary, Secret Ultra Lock)
   â€¢ Batch jobs with XP tracking & leaderboard
   â€¢ Integration with HyperShrink 8000 engine
"""

import asyncio
import hashlib
import json
import logging
import os
import sqlite3
import subprocess
import sys
import time
import zlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# Add the chaosgenius path
sys.path.append("/root/chaosgenius")

try:
    from HyperShrink_8000.hypershrink_8000 import HyperShrinkCore
    HYPERSHRINK_AVAILABLE = True
except ImportError:
    HYPERSHRINK_AVAILABLE = False

try:
    from colorama import Fore, Back, Style, init
    from tqdm import tqdm
    init(autoreset=True)
    STYLING_AVAILABLE = True
except ImportError:
    STYLING_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UltraArchiveBuilderAgent:
    """ğŸ’¾ğŸ§¬ Master Archive Builder Core System ğŸ§¬ğŸ’¾"""

    def __init__(self):
        self.base_path = "/root/chaosgenius"
        self.archive_db = f"{self.base_path}/ultra_archive_builder.db"
        self.archives_dir = f"{self.base_path}/archives"
        self.version_control_dir = f"{self.base_path}/archive_versions"

        # Create directories
        Path(self.archives_dir).mkdir(exist_ok=True)
        Path(self.version_control_dir).mkdir(exist_ok=True)

        # Archive Builder Stats & XP System
        self.stats = {
            "files_archived": 0,
            "bytes_saved": 0,
            "xp_points": 0,
            "archives_created": 0,
            "legendary_compressions": 0,
            "secret_ultra_locks": 0,
            "batch_jobs_completed": 0,
            "total_restore_operations": 0
        }

        # Compression Presets
        self.compression_presets = {
            "FAST_BACKUP": {
                "level": 3,
                "description": "âš¡ Quick backup compression",
                "tags": ["#fast", "#backup"],
                "xp_multiplier": 1.0
            },
            "LEGENDARY_ARCHIVE": {
                "level": 19,
                "description": "ğŸ† Maximum compression power",
                "tags": ["#legendary", "#hyperdoc"],
                "xp_multiplier": 2.5
            },
            "SECRET_ULTRA_LOCK": {
                "level": 22,
                "description": "ğŸ” Ultra secure encrypted archive",
                "tags": ["#encrypt", "#ultra", "#secret"],
                "xp_multiplier": 5.0
            },
            "HYPERFOCUS_MODE": {
                "level": 15,
                "description": "ğŸ§¬ HyperFocus Zone optimization",
                "tags": ["#hyperfocus", "#zone"],
                "xp_multiplier": 3.0
            }
        }

        # Auto-shrink rules
        self.auto_shrink_rules = {
            "large_files": {"size_mb": 50, "auto_compress": True},
            "old_logs": {"age_days": 7, "extensions": [".log", ".tmp"], "auto_compress": True},
            "media_files": {"extensions": [".mp4", ".avi", ".mkv"], "preset": "FAST_BACKUP"},
            "documents": {"extensions": [".pdf", ".docx", ".xlsx"], "preset": "LEGENDARY_ARCHIVE"},
            "code_projects": {"directories": ["src", "dist", "build"], "preset": "HYPERFOCUS_MODE"}
        }

        # Initialize systems
        self.print_legendary_banner()
        self.init_archive_database()
        self.load_stats()

        # Initialize HyperShrink integration
        if HYPERSHRINK_AVAILABLE:
            self.hypershrink = HyperShrinkCore()
            print("ğŸ¤ HyperShrink 8000 integration: ONLINE!")
        else:
            self.hypershrink = None
            print("âš ï¸ HyperShrink 8000 not available - using fallback compression")

    def print_legendary_banner(self):
        """ğŸ”¥ Display the legendary Ultra Archive Builder banner ğŸ”¥"""
        if STYLING_AVAILABLE:
            banner = f"""
{Fore.CYAN}{'='*80}
{Fore.YELLOW}ğŸ’¾ğŸ§¬ ULTRA ARCHIVE BUILDER AGENT - MASTER CORE SYSTEM ğŸ§¬ğŸ’¾
{Fore.MAGENTA}ğŸŒŒ Intelligent Archiving â€¢ Version Control â€¢ XP Tracking ğŸŒŒ
{Fore.GREEN}ğŸ‘‘ By Command of Chief Lyndz - LEGENDARY FILE MASTERY! ğŸ‘‘
{Fore.CYAN}{'='*80}
            """
        else:
            banner = """
================================================================================
ğŸ’¾ğŸ§¬ ULTRA ARCHIVE BUILDER AGENT - MASTER CORE SYSTEM ğŸ§¬ğŸ’¾
ğŸŒŒ Intelligent Archiving â€¢ Version Control â€¢ XP Tracking ğŸŒŒ
ğŸ‘‘ By Command of Chief Lyndz - LEGENDARY FILE MASTERY! ğŸ‘‘
================================================================================
            """
        print(banner)

    def init_archive_database(self):
        """ğŸ—„ï¸ Initialize the Ultra Archive Builder database"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        # Archive Registry Table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS archive_registry (
                archive_id TEXT PRIMARY KEY,
                original_path TEXT,
                archive_path TEXT,
                preset_used TEXT,
                tags TEXT,
                original_size INTEGER,
                compressed_size INTEGER,
                compression_ratio REAL,
                sha256_original TEXT,
                sha256_compressed TEXT,
                created_timestamp REAL,
                metadata TEXT,
                status TEXT DEFAULT 'ACTIVE'
            )
        """)

        # Version Control Table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS version_control (
                version_id TEXT PRIMARY KEY,
                archive_id TEXT,
                version_number INTEGER,
                version_path TEXT,
                changes_description TEXT,
                created_timestamp REAL,
                restore_count INTEGER DEFAULT 0,
                FOREIGN KEY (archive_id) REFERENCES archive_registry (archive_id)
            )
        """)

        # Batch Jobs Table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS batch_jobs (
                job_id TEXT PRIMARY KEY,
                job_name TEXT,
                job_type TEXT,
                status TEXT DEFAULT 'PENDING',
                total_files INTEGER DEFAULT 0,
                processed_files INTEGER DEFAULT 0,
                bytes_saved INTEGER DEFAULT 0,
                xp_earned INTEGER DEFAULT 0,
                started_timestamp REAL,
                completed_timestamp REAL,
                job_config TEXT
            )
        """)

        # XP Leaderboard Table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS xp_leaderboard (
                user_id TEXT PRIMARY KEY,
                username TEXT,
                total_xp INTEGER DEFAULT 0,
                archives_created INTEGER DEFAULT 0,
                legendary_achievements INTEGER DEFAULT 0,
                last_activity REAL,
                achievements TEXT DEFAULT '[]'
            )
        """)

        # Auto-shrink Rules Table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS auto_shrink_rules (
                rule_id TEXT PRIMARY KEY,
                rule_name TEXT,
                rule_config TEXT,
                enabled BOOLEAN DEFAULT TRUE,
                last_executed REAL,
                files_processed INTEGER DEFAULT 0
            )
        """)

        conn.commit()
        conn.close()
        print("ğŸ—„ï¸ Ultra Archive Builder database initialized!")

    def calculate_file_hash(self, file_path: str) -> str:
        """ğŸ” Calculate SHA256 hash for file integrity"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()

    def detect_file_tags(self, file_path: str) -> List[str]:
        """ğŸ·ï¸ Intelligently detect appropriate tags for file"""
        tags = []
        file_path = Path(file_path)

        # Size-based tags
        size_mb = file_path.stat().st_size / (1024 * 1024)
        if size_mb > 100:
            tags.append("#large")
        elif size_mb < 1:
            tags.append("#small")

        # Extension-based tags
        ext = file_path.suffix.lower()
        if ext in ['.txt', '.md', '.doc', '.docx', '.pdf']:
            tags.append("#document")
            tags.append("#hyperdoc")
        elif ext in ['.py', '.js', '.html', '.css', '.cpp', '.java']:
            tags.append("#code")
            tags.append("#hyperfocus")
        elif ext in ['.mp4', '.avi', '.mkv', '.mp3', '.wav']:
            tags.append("#media")
        elif ext in ['.zip', '.tar', '.gz', '.7z']:
            tags.append("#archive")

        # Directory-based tags
        if 'secret' in str(file_path).lower() or 'private' in str(file_path).lower():
            tags.append("#encrypt")
        if 'backup' in str(file_path).lower():
            tags.append("#backup")
        if 'temp' in str(file_path).lower() or 'tmp' in str(file_path).lower():
            tags.append("#temp")

        return tags

    def create_archive(self, target_path: str, preset: str = "LEGENDARY_ARCHIVE",
                      custom_tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """ğŸ“¦ Create archive with specified preset and tags"""
        target_path = Path(target_path)
        if not target_path.exists():
            raise FileNotFoundError(f"Target path not found: {target_path}")

        # Generate archive ID
        archive_id = hashlib.sha256(f"{target_path}_{time.time()}".encode()).hexdigest()[:16]

        # Prepare archive
        preset_config = self.compression_presets[preset]
        auto_tags = self.detect_file_tags(str(target_path))
        all_tags = auto_tags + preset_config["tags"] + (custom_tags or [])

        # Calculate original file info
        original_size = target_path.stat().st_size if target_path.is_file() else self._get_directory_size(target_path)
        original_hash = self.calculate_file_hash(str(target_path)) if target_path.is_file() else "N/A"

        # Create archive filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_filename = f"{target_path.stem}_{preset}_{timestamp}.hfz"
        archive_path = Path(self.archives_dir) / archive_filename

        print(f"ğŸš€ Creating {preset} archive: {target_path.name}")
        if STYLING_AVAILABLE:
            print(f"{Fore.CYAN}ğŸ“Š Original size: {self._format_bytes(original_size)}")
            print(f"{Fore.YELLOW}ğŸ·ï¸ Tags: {', '.join(all_tags)}")

        start_time = time.time()

        # Use HyperShrink if available, otherwise fallback
        if self.hypershrink and target_path.is_file():
            result = self.hypershrink.compress_file(
                str(target_path),
                str(archive_path),
                mode=preset.replace('_', '').upper()
            )
            compressed_size = result['compressed_size']
            compression_ratio = result['compression_ratio']
        else:
            # Fallback compression for directories or when HyperShrink unavailable
            compressed_size, compression_ratio = self._fallback_compress(target_path, archive_path)

        compressed_hash = self.calculate_file_hash(str(archive_path))
        compression_time = time.time() - start_time

        # Calculate XP earned
        base_xp = int(compression_ratio * 10)
        xp_earned = int(base_xp * preset_config["xp_multiplier"])

        # Create metadata
        metadata = {
            "original_name": target_path.name,
            "compression_preset": preset,
            "compression_time": compression_time,
            "xp_earned": xp_earned,
            "created_by": "UltraArchiveBuilder",
            "hypershrink_used": self.hypershrink is not None,
            "file_count": self._get_file_count(target_path) if target_path.is_dir() else 1
        }

        # Store in database
        self._register_archive(
            archive_id, str(target_path), str(archive_path), preset,
            all_tags, original_size, compressed_size, compression_ratio,
            original_hash, compressed_hash, metadata
        )

        # Update stats
        self.stats["files_archived"] += 1
        self.stats["bytes_saved"] += (original_size - compressed_size)
        self.stats["xp_points"] += xp_earned
        self.stats["archives_created"] += 1

        if preset == "LEGENDARY_ARCHIVE" and compression_ratio > 70:
            self.stats["legendary_compressions"] += 1
        elif preset == "SECRET_ULTRA_LOCK":
            self.stats["secret_ultra_locks"] += 1

        self.save_stats()

        # Print results
        self._print_archive_results({
            "archive_id": archive_id,
            "original_size": original_size,
            "compressed_size": compressed_size,
            "compression_ratio": compression_ratio,
            "xp_earned": xp_earned,
            "preset": preset,
            "tags": all_tags,
            "archive_path": str(archive_path)
        })

        return {
            "archive_id": archive_id,
            "archive_path": str(archive_path),
            "original_size": original_size,
            "compressed_size": compressed_size,
            "compression_ratio": compression_ratio,
            "xp_earned": xp_earned,
            "tags": all_tags
        }

    def _get_directory_size(self, directory: Path) -> int:
        """ğŸ“ Calculate total size of directory"""
        total_size = 0
        for item in directory.rglob('*'):
            if item.is_file():
                total_size += item.stat().st_size
        return total_size

    def _get_file_count(self, path: Path) -> int:
        """ğŸ“Š Count files in directory"""
        if path.is_file():
            return 1
        return len(list(path.rglob('*')))

    def _fallback_compress(self, source: Path, target: Path) -> Tuple[int, float]:
        """ğŸ’ª Fallback compression using zlib"""
        import tarfile

        with tarfile.open(target, 'w:gz') as tar:
            if source.is_file():
                tar.add(source, arcname=source.name)
            else:
                tar.add(source, arcname=source.name)

        compressed_size = target.stat().st_size
        original_size = self._get_directory_size(source) if source.is_dir() else source.stat().st_size
        compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0

        return compressed_size, compression_ratio

    def _register_archive(self, archive_id: str, original_path: str, archive_path: str,
                         preset: str, tags: List[str], original_size: int, compressed_size: int,
                         compression_ratio: float, original_hash: str, compressed_hash: str,
                         metadata: Dict[str, Any]):
        """ğŸ“ Register archive in database"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO archive_registry
            (archive_id, original_path, archive_path, preset_used, tags, original_size,
             compressed_size, compression_ratio, sha256_original, sha256_compressed,
             created_timestamp, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            archive_id, original_path, archive_path, preset, json.dumps(tags),
            original_size, compressed_size, compression_ratio, original_hash,
            compressed_hash, time.time(), json.dumps(metadata)
        ))

        conn.commit()
        conn.close()

    def _print_archive_results(self, result: Dict[str, Any]):
        """ğŸ‰ Print beautiful archive creation results"""
        if STYLING_AVAILABLE:
            print(f"\n{Fore.GREEN}ğŸ‰ ARCHIVE CREATION COMPLETE! ğŸ‰")
            print(f"{Fore.YELLOW}ğŸ“¦ Archive ID: {result['archive_id']}")
            print(f"{Fore.CYAN}ğŸ“ Original Size: {self._format_bytes(result['original_size'])}")
            print(f"{Fore.BLUE}ğŸ¤ Compressed Size: {self._format_bytes(result['compressed_size'])}")
            print(f"{Fore.RED}âš¡ Compression Ratio: {result['compression_ratio']:.1f}%")
            print(f"{Fore.MAGENTA}ğŸ† XP Earned: {result['xp_earned']} points!")
            print(f"{Fore.GREEN}ğŸ·ï¸ Tags: {', '.join(result['tags'])}")

            # Special achievements
            if result['compression_ratio'] > 80:
                print(f"{Fore.MAGENTA}ğŸ§¬ LEGENDARY COMPRESSION ACHIEVED! HYPERFOCUS BONUS! ğŸ§¬")
            if result['preset'] == "SECRET_ULTRA_LOCK":
                print(f"{Fore.RED}ğŸ” SECRET ULTRA LOCK DEPLOYED! MAXIMUM SECURITY! ğŸ”")
        else:
            print(f"\nğŸ‰ ARCHIVE CREATION COMPLETE!")
            print(f"ğŸ“¦ Archive ID: {result['archive_id']}")
            print(f"ğŸ“ Original Size: {self._format_bytes(result['original_size'])}")
            print(f"ğŸ¤ Compressed Size: {self._format_bytes(result['compressed_size'])}")
            print(f"âš¡ Compression Ratio: {result['compression_ratio']:.1f}%")
            print(f"ğŸ† XP Earned: {result['xp_earned']} points!")

    def _format_bytes(self, bytes_value: int) -> str:
        """ğŸ“Š Format bytes into human readable format"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_value < 1024.0:
                return f"{bytes_value:.1f} {unit}"
            bytes_value /= 1024.0
        return f"{bytes_value:.1f} PB"

    def auto_shrink_directory(self, directory: str, rules: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ğŸ¤– Automatically shrink directory based on rules"""
        directory = Path(directory)
        if not directory.is_dir():
            raise ValueError(f"Path is not a directory: {directory}")

        rules = rules or self.auto_shrink_rules
        shrink_results = {
            "total_files_processed": 0,
            "archives_created": 0,
            "bytes_saved": 0,
            "xp_earned": 0,
            "operations": []
        }

        print(f"ğŸ¤– Auto-shrinking directory: {directory}")

        for file_path in directory.rglob('*'):
            if not file_path.is_file():
                continue

            file_processed = False

            # Check each rule
            for rule_name, rule_config in rules.items():
                if self._file_matches_rule(file_path, rule_config):
                    print(f"ğŸ“‹ Rule '{rule_name}' matched: {file_path.name}")

                    # Determine preset
                    preset = rule_config.get("preset", "FAST_BACKUP")

                    # Create archive
                    result = self.create_archive(str(file_path), preset, [f"#auto_{rule_name}"])

                    shrink_results["operations"].append({
                        "file": str(file_path),
                        "rule": rule_name,
                        "archive_id": result["archive_id"],
                        "compression_ratio": result["compression_ratio"],
                        "xp_earned": result["xp_earned"]
                    })

                    shrink_results["archives_created"] += 1
                    shrink_results["bytes_saved"] += (result["original_size"] - result["compressed_size"])
                    shrink_results["xp_earned"] += result["xp_earned"]

                    file_processed = True
                    break

            if file_processed:
                shrink_results["total_files_processed"] += 1

        print(f"ğŸ‰ Auto-shrink complete! Processed {shrink_results['total_files_processed']} files")
        return shrink_results

    def _file_matches_rule(self, file_path: Path, rule_config: Dict[str, Any]) -> bool:
        """ğŸ” Check if file matches auto-shrink rule"""
        # Size check
        if "size_mb" in rule_config:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            if file_size_mb < rule_config["size_mb"]:
                return False

        # Age check
        if "age_days" in rule_config:
            file_age = time.time() - file_path.stat().st_mtime
            if file_age < (rule_config["age_days"] * 24 * 3600):
                return False

        # Extension check
        if "extensions" in rule_config:
            if file_path.suffix.lower() not in rule_config["extensions"]:
                return False

        # Directory check
        if "directories" in rule_config:
            if not any(dir_name in str(file_path) for dir_name in rule_config["directories"]):
                return False

        return rule_config.get("auto_compress", True)

    def create_batch_job(self, job_name: str, target_paths: List[str],
                        preset: str = "LEGENDARY_ARCHIVE") -> str:
        """ğŸ“‹ Create batch compression job"""
        job_id = hashlib.sha256(f"{job_name}_{time.time()}".encode()).hexdigest()[:16]

        job_config = {
            "target_paths": target_paths,
            "preset": preset,
            "batch_size": len(target_paths)
        }

        # Register job in database
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO batch_jobs
            (job_id, job_name, job_type, total_files, started_timestamp, job_config)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (job_id, job_name, "BATCH_COMPRESS", len(target_paths), time.time(), json.dumps(job_config)))
        conn.commit()
        conn.close()

        print(f"ğŸ“‹ Batch job created: {job_name} ({len(target_paths)} files)")
        return job_id

    async def execute_batch_job(self, job_id: str) -> Dict[str, Any]:
        """ğŸš€ Execute batch compression job"""
        # Get job details
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()
        job_data = cursor.execute(
            "SELECT * FROM batch_jobs WHERE job_id = ?", (job_id,)
        ).fetchone()
        conn.close()

        if not job_data:
            raise ValueError(f"Batch job not found: {job_id}")

        job_config = json.loads(job_data[10])  # job_config column
        target_paths = job_config["target_paths"]
        preset = job_config["preset"]

        print(f"ğŸš€ Executing batch job: {job_data[1]}")  # job_name

        batch_results = {
            "job_id": job_id,
            "processed_files": 0,
            "failed_files": 0,
            "total_bytes_saved": 0,
            "total_xp_earned": 0,
            "archive_ids": []
        }

        # Process files with progress
        if STYLING_AVAILABLE:
            progress_bar = tqdm(target_paths, desc="ğŸ”¥ Batch Processing", colour='cyan')
        else:
            progress_bar = target_paths

        for file_path in progress_bar:
            try:
                result = self.create_archive(file_path, preset, [f"#batch_{job_id[:8]}"])
                batch_results["archive_ids"].append(result["archive_id"])
                batch_results["total_bytes_saved"] += (result["original_size"] - result["compressed_size"])
                batch_results["total_xp_earned"] += result["xp_earned"]
                batch_results["processed_files"] += 1

                # Update job progress
                self._update_batch_job_progress(job_id, batch_results["processed_files"],
                                              batch_results["total_bytes_saved"],
                                              batch_results["total_xp_earned"])

                # Small delay to prevent system overload
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"Failed to process {file_path}: {e}")
                batch_results["failed_files"] += 1

        # Mark job as completed
        self._complete_batch_job(job_id)
        self.stats["batch_jobs_completed"] += 1
        self.save_stats()

        print(f"ğŸ‰ Batch job completed! {batch_results['processed_files']} files processed")
        return batch_results

    def _update_batch_job_progress(self, job_id: str, processed_files: int,
                                  bytes_saved: int, xp_earned: int):
        """ğŸ“Š Update batch job progress"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE batch_jobs
            SET processed_files = ?, bytes_saved = ?, xp_earned = ?
            WHERE job_id = ?
        """, (processed_files, bytes_saved, xp_earned, job_id))
        conn.commit()
        conn.close()

    def _complete_batch_job(self, job_id: str):
        """âœ… Mark batch job as completed"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE batch_jobs
            SET status = 'COMPLETED', completed_timestamp = ?
            WHERE job_id = ?
        """, (time.time(), job_id))
        conn.commit()
        conn.close()

    def create_version(self, archive_id: str, changes_description: str = "") -> str:
        """ğŸ”„ Create new version of existing archive"""
        # Get original archive info
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()
        archive_data = cursor.execute(
            "SELECT * FROM archive_registry WHERE archive_id = ?", (archive_id,)
        ).fetchone()

        if not archive_data:
            raise ValueError(f"Archive not found: {archive_id}")

        original_path = archive_data[1]

        # Get next version number
        version_count = cursor.execute(
            "SELECT COUNT(*) FROM version_control WHERE archive_id = ?", (archive_id,)
        ).fetchone()[0]

        version_number = version_count + 1
        version_id = f"{archive_id}_v{version_number}"

        conn.close()

        # Create new archive (version)
        if Path(original_path).exists():
            result = self.create_archive(original_path,
                                       archive_data[3],  # preset_used
                                       [f"#version_{version_number}"])

            # Register version
            conn = sqlite3.connect(self.archive_db)
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO version_control
                (version_id, archive_id, version_number, version_path,
                 changes_description, created_timestamp)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (version_id, archive_id, version_number, result["archive_path"],
                  changes_description, time.time()))
            conn.commit()
            conn.close()

            print(f"ğŸ”„ Version {version_number} created for archive {archive_id[:8]}")
            return version_id
        else:
            raise FileNotFoundError(f"Original file no longer exists: {original_path}")

    def restore_archive(self, archive_id: str, restore_path: Optional[str] = None,
                       version_number: Optional[int] = None) -> Dict[str, Any]:
        """ğŸ“¦ Restore archive to original or specified location"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        if version_number:
            # Restore specific version
            version_data = cursor.execute("""
                SELECT v.*, a.original_path FROM version_control v
                JOIN archive_registry a ON v.archive_id = a.archive_id
                WHERE v.archive_id = ? AND v.version_number = ?
            """, (archive_id, version_number)).fetchone()

            if not version_data:
                raise ValueError(f"Version {version_number} not found for archive {archive_id}")

            archive_path = version_data[3]  # version_path
            original_path = version_data[6]  # original_path from join

            # Increment restore count for version
            cursor.execute("""
                UPDATE version_control SET restore_count = restore_count + 1
                WHERE version_id = ?
            """, (version_data[0],))  # version_id

        else:
            # Restore latest archive
            archive_data = cursor.execute(
                "SELECT * FROM archive_registry WHERE archive_id = ?", (archive_id,)
            ).fetchone()

            if not archive_data:
                raise ValueError(f"Archive not found: {archive_id}")

            archive_path = archive_data[2]  # archive_path
            original_path = archive_data[1]  # original_path

        conn.commit()
        conn.close()

        # Determine restore location
        if restore_path is None:
            restore_path = original_path

        restore_path = Path(restore_path)
        archive_path = Path(archive_path)

        print(f"ğŸ“¦ Restoring archive {archive_id[:8]} to {restore_path}")

        start_time = time.time()

        # Use HyperShrink decompression if available
        if self.hypershrink and archive_path.suffix == '.hfz':
            result = self.hypershrink.decompress_file(str(archive_path), str(restore_path))
            restore_time = result.get('time_taken', time.time() - start_time)
        else:
            # Fallback decompression
            restore_time = self._fallback_decompress(archive_path, restore_path)

        # Update stats
        self.stats["total_restore_operations"] += 1
        self.stats["xp_points"] += 25  # Bonus XP for restore
        self.save_stats()

        restore_result = {
            "archive_id": archive_id,
            "restore_path": str(restore_path),
            "restore_time": restore_time,
            "version_restored": version_number,
            "xp_earned": 25
        }

        print(f"âœ… Archive restored successfully in {restore_time:.2f}s")
        print(f"ğŸ† +25 XP earned for restore operation!")

        return restore_result

    def _fallback_decompress(self, archive_path: Path, restore_path: Path) -> float:
        """ğŸ”„ Fallback decompression using tarfile"""
        import tarfile

        start_time = time.time()

        with tarfile.open(archive_path, 'r:gz') as tar:
            tar.extractall(path=restore_path.parent)

        return time.time() - start_time

    def get_archive_stats(self) -> Dict[str, Any]:
        """ğŸ“Š Get comprehensive archive statistics"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        # Get database stats
        total_archives = cursor.execute("SELECT COUNT(*) FROM archive_registry").fetchone()[0]
        total_versions = cursor.execute("SELECT COUNT(*) FROM version_control").fetchone()[0]
        total_batch_jobs = cursor.execute("SELECT COUNT(*) FROM batch_jobs").fetchone()[0]

        # Get compression stats
        compression_stats = cursor.execute("""
            SELECT
                AVG(compression_ratio) as avg_ratio,
                MAX(compression_ratio) as max_ratio,
                SUM(original_size - compressed_size) as total_saved
            FROM archive_registry
        """).fetchone()

        # Get preset usage
        preset_usage = cursor.execute("""
            SELECT preset_used, COUNT(*) as count
            FROM archive_registry
            GROUP BY preset_used
        """).fetchall()

        # Get recent activity
        recent_archives = cursor.execute("""
            SELECT COUNT(*) FROM archive_registry
            WHERE created_timestamp > ?
        """, (time.time() - 86400,)).fetchone()[0]  # Last 24 hours

        conn.close()

        stats = {
            "ğŸ¤– Agent Status": "ULTRA ARCHIVE BUILDER ONLINE",
            "ğŸ“¦ Total Archives": total_archives,
            "ğŸ”„ Version Control": f"{total_versions} versions tracked",
            "ğŸ“‹ Batch Jobs": f"{total_batch_jobs} completed",
            "ğŸ† Total XP": self.stats["xp_points"],
            "ğŸ’¾ Bytes Saved": self._format_bytes(self.stats["bytes_saved"]),
            "âš¡ Avg Compression": f"{compression_stats[0]:.1f}%" if compression_stats[0] else "0%",
            "ğŸ¥‡ Best Compression": f"{compression_stats[1]:.1f}%" if compression_stats[1] else "0%",
            "ğŸ• Recent Activity": f"{recent_archives} archives (24h)",
            "ğŸ”¥ Legendary Archives": self.stats["legendary_compressions"],
            "ğŸ” Secret Ultra Locks": self.stats["secret_ultra_locks"],
            "ğŸ“Š Preset Usage": dict(preset_usage)
        }

        return stats

    def get_leaderboard(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ğŸ† Get XP leaderboard"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        leaderboard = cursor.execute("""
            SELECT username, total_xp, archives_created, legendary_achievements
            FROM xp_leaderboard
            ORDER BY total_xp DESC
            LIMIT ?
        """, (limit,)).fetchall()

        conn.close()

        return [
            {
                "rank": i + 1,
                "username": row[0],
                "xp": row[1],
                "archives": row[2],
                "legendary": row[3]
            }
            for i, row in enumerate(leaderboard)
        ]

    def search_archives(self, query: str, tag_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """ğŸ” Search archives by filename, tags, or metadata"""
        conn = sqlite3.connect(self.archive_db)
        cursor = conn.cursor()

        sql = """
            SELECT archive_id, original_path, tags, compression_ratio, created_timestamp
            FROM archive_registry
            WHERE (original_path LIKE ? OR tags LIKE ?)
        """
        params = [f"%{query}%", f"%{query}%"]

        if tag_filter:
            sql += " AND tags LIKE ?"
            params.append(f"%{tag_filter}%")

        sql += " ORDER BY created_timestamp DESC"

        results = cursor.execute(sql, params).fetchall()
        conn.close()

        return [
            {
                "archive_id": row[0],
                "original_path": row[1],
                "tags": json.loads(row[2]),
                "compression_ratio": row[3],
                "created": datetime.fromtimestamp(row[4]).strftime("%Y-%m-%d %H:%M")
            }
            for row in results
        ]

    def load_stats(self):
        """ğŸ“Š Load statistics from database or file"""
        stats_file = Path(self.base_path) / 'ultra_archive_stats.json'
        if stats_file.exists():
            try:
                with open(stats_file, 'r') as f:
                    saved_stats = json.load(f)
                    self.stats.update(saved_stats)
            except:
                pass

    def save_stats(self):
        """ğŸ’¾ Save statistics to file"""
        stats_file = Path(self.base_path) / 'ultra_archive_stats.json'
        try:
            with open(stats_file, 'w') as f:
                json.dump(self.stats, f, indent=2)
        except:
            pass

    def show_dashboard(self):
        """ğŸ“Š Display comprehensive dashboard"""
        stats = self.get_archive_stats()
        leaderboard = self.get_leaderboard(5)

        if STYLING_AVAILABLE:
            print(f"\n{Fore.CYAN}{'='*80}")
            print(f"{Fore.YELLOW}ğŸ’¾ğŸ§¬ ULTRA ARCHIVE BUILDER - COMMAND DASHBOARD ğŸ§¬ğŸ’¾")
            print(f"{Fore.CYAN}{'='*80}")

            for key, value in stats.items():
                print(f"{Fore.GREEN}{key}: {Fore.WHITE}{value}")

            print(f"\n{Fore.MAGENTA}ğŸ† TOP ARCHIVERS:")
            for entry in leaderboard:
                print(f"{Fore.YELLOW}#{entry['rank']} {entry['username']}: {entry['xp']} XP")

        else:
            print("\n" + "="*80)
            print("ğŸ’¾ğŸ§¬ ULTRA ARCHIVE BUILDER - COMMAND DASHBOARD ğŸ§¬ğŸ’¾")
            print("="*80)

            for key, value in stats.items():
                print(f"{key}: {value}")

            print("\nğŸ† TOP ARCHIVERS:")
            for entry in leaderboard:
                print(f"#{entry['rank']} {entry['username']}: {entry['xp']} XP")


def main():
    """ğŸš€ Launch Ultra Archive Builder Agent"""
    import argparse

    parser = argparse.ArgumentParser(description="ğŸ’¾ Ultra Archive Builder Agent")
    parser.add_argument('command', choices=['create', 'restore', 'batch', 'auto-shrink', 'stats', 'search', 'dashboard'],
                       help='Command to execute')
    parser.add_argument('target', nargs='?', help='Target file/directory path')
    parser.add_argument('-p', '--preset', choices=['FAST_BACKUP', 'LEGENDARY_ARCHIVE', 'SECRET_ULTRA_LOCK', 'HYPERFOCUS_MODE'],
                       default='LEGENDARY_ARCHIVE', help='Compression preset')
    parser.add_argument('-t', '--tags', nargs='*', help='Custom tags for archive')
    parser.add_argument('-o', '--output', help='Output path for restore')
    parser.add_argument('-v', '--version', type=int, help='Version number for restore')
    parser.add_argument('-q', '--query', help='Search query')
    parser.add_argument('--tag-filter', help='Filter by tag')

    args = parser.parse_args()

    # Create Ultra Archive Builder instance
    archive_builder = UltraArchiveBuilderAgent()

    try:
        if args.command == 'dashboard':
            archive_builder.show_dashboard()

        elif args.command == 'stats':
            stats = archive_builder.get_archive_stats()
            for key, value in stats.items():
                print(f"{key}: {value}")

        elif args.command == 'create':
            if not args.target:
                print("âŒ Error: Target path required for archive creation")
                return 1
            result = archive_builder.create_archive(args.target, args.preset, args.tags)
            print(f"âœ… Archive created: {result['archive_id']}")

        elif args.command == 'restore':
            if not args.target:
                print("âŒ Error: Archive ID required for restore")
                return 1
            result = archive_builder.restore_archive(args.target, args.output, args.version)
            print(f"âœ… Archive restored to: {result['restore_path']}")

        elif args.command == 'auto-shrink':
            if not args.target:
                print("âŒ Error: Directory path required for auto-shrink")
                return 1
            result = archive_builder.auto_shrink_directory(args.target)
            print(f"âœ… Auto-shrink complete: {result['archives_created']} archives created")

        elif args.command == 'search':
            if not args.query:
                print("âŒ Error: Search query required")
                return 1
            results = archive_builder.search_archives(args.query, args.tag_filter)
            for result in results:
                print(f"ğŸ“¦ {result['archive_id'][:8]}: {result['original_path']} ({result['compression_ratio']:.1f}%)")

        return 0

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return 1


if __name__ == "__main__":
    sys.exit(main())